{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучаем Word2vec: практикум по созданию векторных моделей языка\n",
    "\n",
    "**Word2vec** - библиотека для получения векторных представлений слов на основе их совместной встречаемости в текстах. Системный Блокъ уже писал ранее о том, как работают эти модели, и вы можете освежить в памяти механизмы работы **word2vec**, прочитав [эту статью](https://vk.com/@sysblok-word2vec-pokazhi-mne-svoi-kontekst-i-ya-skazhu-kto-ty). \n",
    "\n",
    "Сейчас мы займемся более практичными и приземленными вещами: научимся использовать **word2vec** в своей повседневной работе. Мы будем использовать реализацию **word2vec** в библиотеке **gensim** для языка программирования **python**.\n",
    "\n",
    "Тьюториал состоит из двух частей:\n",
    "* В первой части мы научимся предобрабатывать текстовые файлы и самостоятельно тренировать векторную модель на своих данных.\n",
    "* Во второй части мы разберёмся, как загружать уже готовые векторные модели и работать с ними. Например, мы научимся выполнять простые операции над векторами слов, такие как «найти слово с наиболее близким вектором» или «вычислить коэффициент близости между двумя векторами слов». Также мы рассмотрим более сложные операции над векторами, например, «найти семантические аналоги» или «найти лишний вектор в группе слов».\n",
    "\n",
    "Для прохождения тьюториала мы рекомендуем использовать **python3**. Работоспособность кода для **python2** не гарантируется."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Предобработка текстовых данных и тренировка модели\n",
    "\n",
    "Перед тем, как переходить к тренировке моделей, бывает необходимо привести данные в формат, удобный для работы. Тексты обычно очищаются от пунктуации, приводятся к нижнему регистру. Этот процесс называется предобработкой текстовых данных. Для того, чтобы начать тренировать векторную модель при помощи **word2vec**, предобработку текста выполнять не обязательно. Для того, чтобы алгоритм смог выучить вектора слов, достаточно разделить текст на предложения. \n",
    "\n",
    "Однако в нашем тьюториале мы разберём, как выполнять более глубокую предобработку текста: лемматизацию и частеречный анализ. Это нужно для того, чтобы самим создавать модели, совместимые с уже готовыми [моделями RusVectōrēs](https://rusvectores.org/ru/models/). Ну и просто знание о том, как устроена предобработка текста, может пригодиться во многих задачах обработки языка.\n",
    "\n",
    "### Обработка текста\n",
    "\n",
    "Предобработка текстов для тренировки моделей выглядит следующим образом:\n",
    "* сначала мы приведем все слова к начальной форме (лемматизируем) и удалим стоп-слова;\n",
    "* затем мы приведем все леммы к нижнему регистру;\n",
    "* для каждого слова добавим его частеречный тэг.\n",
    "\n",
    "Давайте попробуем воссоздать процесс предобработки текста на примере рассказа [О. Генри \"Русские соболя\"](https://rusvectores.org/static/henry_sobolya.txt). Для предобработки можно использовать различные тэггеры, мы сейчас будем использовать [*UDPipe*](https://ufal.mff.cuni.cz/udpipe), чтобы сразу получить частеречную разметку в виде Universal POS-tags. Сначала установим обертку *UDPipe* для Python:\n",
    "\n",
    "`pip install ufal.udpipe`\n",
    "\n",
    "*UDPipe* использует предобученные модели для лемматизации и тэггинга. Вы можете использовать [уже готовую модель](https://rusvectores.org/static/models/udpipe_syntagrus.model) или обучить свою. \n",
    "\n",
    "Чтобы загружать файлы, можно использовать питоновскую библиотеку **wget**:\n",
    "\n",
    "`pip install wget`\n",
    "\n",
    "Кусок кода ниже скачает рассказ О’Генри и модель UDPipe для лингвистической предобработки. Модель весит 40 мегабайт, поэтому ячейка может выполнятся некоторое время, особенно если у вас небыстрый интернет. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget\n",
    "import sys\n",
    "\n",
    "udpipe_url = 'https://rusvectores.org/static/models/udpipe_syntagrus.model'\n",
    "text_url = 'https://rusvectores.org/static/henry_sobolya.txt'\n",
    "\n",
    "modelfile = wget.download(udpipe_url)\n",
    "textfile = wget.download(text_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Приступим к собственно предобработке текста. Попробуем лемматизировать текст и добавить частеречные тэги при помощи этой функции:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(pipeline, text='Строка', keep_pos=True, keep_punct=False):\n",
    "    entities = {'PROPN'}\n",
    "    named = False\n",
    "    memory = []\n",
    "    mem_case = None\n",
    "    mem_number = None\n",
    "    tagged_propn = []\n",
    "\n",
    "    # обрабатываем текст, получаем результат в формате conllu:\n",
    "    processed = pipeline.process(text)\n",
    "\n",
    "    # пропускаем строки со служебной информацией:\n",
    "    content = [l for l in processed.split('\\n') if not l.startswith('#')]\n",
    "\n",
    "    # извлекаем из обработанного текста леммы, тэги и морфологические характеристики\n",
    "    tagged = [w.split('\\t') for w in content if w]\n",
    "\n",
    "    for t in tagged:\n",
    "        if len(t) != 10:\n",
    "            continue\n",
    "        (word_id, token, lemma, pos, xpos, feats, head, deprel, deps, misc) = t\n",
    "        if not lemma or not token:\n",
    "            continue\n",
    "        if pos in entities:\n",
    "            if '|' not in feats:\n",
    "                tagged_propn.append('%s_%s' % (lemma, pos))\n",
    "                continue\n",
    "            morph = {el.split('=')[0]: el.split('=')[1] for el in feats.split('|')}\n",
    "            if 'Case' not in morph or 'Number' not in morph:\n",
    "                tagged_propn.append('%s_%s' % (lemma, pos))\n",
    "                continue\n",
    "            if not named:\n",
    "                named = True\n",
    "                mem_case = morph['Case']\n",
    "                mem_number = morph['Number']\n",
    "            if morph['Case'] == mem_case and morph['Number'] == mem_number:\n",
    "                memory.append(lemma)\n",
    "                if 'SpacesAfter=\\\\n' in misc or 'SpacesAfter=\\s\\\\n' in misc:\n",
    "                    named = False\n",
    "                    past_lemma = '::'.join(memory)\n",
    "                    memory = []\n",
    "                    tagged_propn.append(past_lemma + '_PROPN ')\n",
    "            else:\n",
    "                named = False\n",
    "                past_lemma = '::'.join(memory)\n",
    "                memory = []\n",
    "                tagged_propn.append(past_lemma + '_PROPN ')\n",
    "                tagged_propn.append('%s_%s' % (lemma, pos))\n",
    "        else:\n",
    "            if not named:\n",
    "                if pos == 'NUM' and token.isdigit():  # Заменяем числа на xxxxx той же длины\n",
    "                    lemma = num_replace(token)\n",
    "                tagged_propn.append('%s_%s' % (lemma, pos))\n",
    "            else:\n",
    "                named = False\n",
    "                past_lemma = '::'.join(memory)\n",
    "                memory = []\n",
    "                tagged_propn.append(past_lemma + '_PROPN ')\n",
    "                tagged_propn.append('%s_%s' % (lemma, pos))\n",
    "\n",
    "    if not keep_punct:\n",
    "        tagged_propn = [word for word in tagged_propn if word.split('_')[1] != 'PUNCT']\n",
    "    if not keep_pos:\n",
    "        tagged_propn = [word.split('_')[0] for word in tagged_propn]\n",
    "    return tagged_propn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Эту функцию можно также изменить под конкретную задачу. Например, если частеречные тэги нам не нужны, в функции ниже выставим `keep_pos=False`. Если необходимо сохранить знаки пунктуации, можно выставить `keep_punct=True`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь загружаем модель *UDPipe*, читаем текстовый файл и обрабатываем его при помощи нашей функции. В файле должен содержаться необработанный текст (одно предложение на строку или один абзац на строку).\n",
    "Этот текст токенизируется, лемматизируется и размечается по частям речи с использованием UDPipe.\n",
    "На выход мы получаем последовательность разделенных пробелами лемм с частями речи (\"зеленый\\_NOUN трамвай\\_NOUN\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ufal.udpipe import Model, Pipeline\n",
    "import os\n",
    "import re\n",
    "\n",
    "def tag_ud(text='Текст нужно передать функции в виде строки!', modelfile='udpipe_syntagrus.model'):\n",
    "    udpipe_model_url = 'https://rusvectores.org/static/models/udpipe_syntagrus.model'\n",
    "    udpipe_filename = udpipe_model_url.split('/')[-1]\n",
    "\n",
    "    if not os.path.isfile(modelfile):\n",
    "        print('UDPipe model not found. Downloading...', file=sys.stderr)\n",
    "        wget.download(udpipe_model_url)\n",
    "\n",
    "    print('\\nLoading the model...', file=sys.stderr)\n",
    "    model = Model.load(modelfile)\n",
    "    process_pipeline = Pipeline(model, 'tokenize', Pipeline.DEFAULT, Pipeline.DEFAULT, 'conllu')\n",
    "\n",
    "    print('Processing input...', file=sys.stderr)\n",
    "    lines = text.split('\\n')\n",
    "    tagged = []\n",
    "    for line in lines:\n",
    "        # line = unify_sym(line.strip()) # здесь могла бы быть ваша функция очистки текста\n",
    "        output = process(process_pipeline, text=line)\n",
    "        tagged_line = ' '.join(output)\n",
    "        tagged.append(tagged_line)\n",
    "    return '\\n'.join(tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading the model...\n",
      "Processing input...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "русский_PROPN  соболь_NOUN о.::генри_PROPN \n",
      "когда_SCONJ синий_ADJ как_SCONJ ночь_NOUN глаз_NOUN Молли_VERB Мак-Кивер_PROPN  класть_VERB малыш::Брэди_PROPN  на_ADP оба_NUM лопатка_NOUN он_PRON вынужденный_ADJ быть_AUX покидать_VERB ряд_NOUN банда_NOUN «Дымовый_ADJ труба»_NOUN таков_ADJ власть_NOUN нежный_ADJ укор_NOUN подружка_NOUN и_CCONJ она_PRON \n"
     ]
    }
   ],
   "source": [
    "text = open(textfile, 'r', encoding='utf-8').read()\n",
    "processed_text = tag_ud(text=text, modelfile=modelfile)\n",
    "print(processed_text[:350])\n",
    "with open('my_text.txt', 'w', encoding='utf-8') as out:\n",
    "    out.write(processed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наша функция напечатает обработанный текст, который мы теперь можем также сохранить в файл. \n",
    "\n",
    "Итак, в ходе этой части тьюториала мы научились от \"сырого текста\" приходить к лемматизированному тексту с частеречными тэгами, который уже можно подавать на вход модели! Теперь теперь попробуем натренировать векторную модель.\n",
    "\n",
    "### Процесс тренировки модели \n",
    "\n",
    "Для работы с эмбеддингами слов существуют различные библиотеки: [gensim](https://radimrehurek.com/gensim/), [keras](https://keras.io/), [tensorflow](https://www.tensorflow.org/), [pytorch](https://pytorch.org/). Мы будем работать с библиотекой *gensim*.\n",
    "\n",
    "\n",
    "***Gensim***  - изначально библиотека для тематического моделирования текстов. Однако помимо различных алгоритмов для *topic modeling* в ней реализованы на **python** и алгоритмы из тулкита **word2vec** (который в оригинале был написан на C++). Прежде всего, если **gensim** у вас на компьютере не установлен, нужно его установить:\n",
    "\n",
    "`pip install gensim`\n",
    "\n",
    "Gensim регулярно обновляется, так что не будет лишним удостовериться, что у вас установлена последняя версия, а при необходимости проапдейтить библиотеку:\n",
    "\n",
    "`pip install gensim --upgrade` \n",
    "\n",
    "или \n",
    "\n",
    "`pip install gensim -U`\n",
    "\n",
    "При подготовке этого тьюториала использовался **gensim** версии 3.7.0.\n",
    "\n",
    "Поскольку обучение и загрузка моделей могут занимать продолжительное время, иногда бывает полезно вести лог событий. Для этого используется стандартная питоновская библиотека **logging**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gensim, logging\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На вход модели мы даем наш обработанный текстовый файл (либо любой другой текст, важно лишь, что каждое предложение должно быть на отдельной строчке)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = 'my_text.txt'\n",
    "data = gensim.models.word2vec.LineSentence(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Инициализируем модель. Параметры в скобочках:\n",
    "* data - данные, \n",
    "* size - размер вектора, \n",
    "* window - размер окна наблюдения,\n",
    "* min_count - мин. частотность слова в корпусе, которое мы берем,\n",
    "* sg - используемый алгоритм обучение (0 - CBOW, 1 - Skip-gram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-01 18:00:29,028 : INFO : collecting all words and their counts\n",
      "2020-10-01 18:00:29,120 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2020-10-01 18:00:29,174 : INFO : collected 981 word types from a corpus of 2085 raw words and 65 sentences\n",
      "2020-10-01 18:00:29,175 : INFO : Loading a fresh vocabulary\n",
      "2020-10-01 18:00:29,178 : INFO : effective_min_count=2 retains 252 unique words (25% of original 981, drops 729)\n",
      "2020-10-01 18:00:29,179 : INFO : effective_min_count=2 leaves 1356 word corpus (65% of original 2085, drops 729)\n",
      "2020-10-01 18:00:29,186 : INFO : deleting the raw counts dictionary of 981 items\n",
      "2020-10-01 18:00:29,188 : INFO : sample=0.001 downsamples 89 most-common words\n",
      "2020-10-01 18:00:29,189 : INFO : downsampling leaves estimated 801 word corpus (59.1% of prior 1356)\n",
      "2020-10-01 18:00:29,214 : INFO : estimated required memory for 252 words and 500 dimensions: 1134000 bytes\n",
      "2020-10-01 18:00:29,215 : INFO : resetting layer weights\n",
      "2020-10-01 18:00:29,309 : INFO : training model with 3 workers on 252 vocabulary and 500 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2020-10-01 18:00:29,317 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-10-01 18:00:29,319 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-10-01 18:00:29,322 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-10-01 18:00:29,324 : INFO : EPOCH - 1 : training on 2085 raw words (785 effective words) took 0.0s, 74843 effective words/s\n",
      "2020-10-01 18:00:29,331 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-10-01 18:00:29,333 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-10-01 18:00:29,337 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-10-01 18:00:29,339 : INFO : EPOCH - 2 : training on 2085 raw words (816 effective words) took 0.0s, 74557 effective words/s\n",
      "2020-10-01 18:00:29,346 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-10-01 18:00:29,348 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-10-01 18:00:29,353 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-10-01 18:00:29,355 : INFO : EPOCH - 3 : training on 2085 raw words (814 effective words) took 0.0s, 67711 effective words/s\n",
      "2020-10-01 18:00:29,363 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-10-01 18:00:29,364 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-10-01 18:00:29,370 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-10-01 18:00:29,371 : INFO : EPOCH - 4 : training on 2085 raw words (815 effective words) took 0.0s, 70772 effective words/s\n",
      "2020-10-01 18:00:29,378 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-10-01 18:00:29,380 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-10-01 18:00:29,386 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-10-01 18:00:29,387 : INFO : EPOCH - 5 : training on 2085 raw words (791 effective words) took 0.0s, 63826 effective words/s\n",
      "2020-10-01 18:00:29,389 : INFO : training on a 10425 raw words (4021 effective words) took 0.1s, 51326 effective words/s\n",
      "2020-10-01 18:00:29,390 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    }
   ],
   "source": [
    "model = gensim.models.Word2Vec(data, size=500, window=10, min_count=2, sg=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы создаем модель, в которой размерность векторов — 500, размер окна наблюдения — 10 слов, алгоритм обучения — CBOW, слова, встретившиеся в корпусе только 1 раз, не используются. После тренировки модели можно нормализовать вектора, тогда модель будет занимать меньше RAM. Однако после этого её нельзя дотренировать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-01 18:00:32,571 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    }
   ],
   "source": [
    "model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Смотрим, сколько в модели слов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "252\n"
     ]
    }
   ],
   "source": [
    "print(len(model.wv.vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И сохраняем!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-01 18:00:34,203 : INFO : saving Word2Vec object under my.model, separately None\n",
      "2020-10-01 18:00:34,206 : INFO : not storing attribute vectors_norm\n",
      "2020-10-01 18:00:34,208 : INFO : not storing attribute cum_table\n",
      "2020-10-01 18:00:34,226 : INFO : saved my.model\n"
     ]
    }
   ],
   "source": [
    "model.save('my.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Работа с векторными моделями при помощи библиотеки Gensim\n",
    "\n",
    "Для своих индивидуальных нужд и экспериментов бывает полезно самому натренировать модель на нужных данных и с нужными параметрами. Но для каких-то общих целей модели уже есть как для русского языка, так и для английского.\n",
    "\n",
    "Модели для русского скачать можно здесь - https://rusvectores.org/ru/models/\n",
    "\n",
    "Существуют несколько форматов, в которых могут храниться модели. Во-первых, данные могут храниться в нативном формате *word2vec*, при этом модель может быть бинарной или не бинарной. Для загрузки модели в формате *word2vec* в классе `KeyedVectors` (в котором хранится большинство относящихся к дистрибутивным моделям функций) существует функция `load_word2vec_format`, а бинарность модели можно указать в аргументе `binary` (внизу будет пример). Помимо этого, модель можно хранить и в собственном формате *gensim*, для этого существует класс `Word2Vec` с функцией `load`. Поскольку модели бывают разных форматов, то для них написаны разные функции загрузки; бывает полезно учитывать это в своем скрипте. Наш код определяет тип модели по её расширению, но вообще файл с моделью может называться как угодно, жестких ограничений для расширения нет.\n",
    "\n",
    "Давайте скачаем новейшую модель для русского языка, созданную на основе [Национального корпуса русского языка (НКРЯ)](http://www.ruscorpora.ru/), и загрузим в её в память (поскольку zip-архив с моделью весит почти 500 мегабайт, следующая ячейка выполнится у вас не сразу!). Распаковывать скачанный архив для обычных моделей не нужно, так как его содержимое прочитается при помощи специальной инструкции:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-01 18:01:02,813 : INFO : loading projection weights from <zipfile.ZipExtFile name='model.bin' mode='r' compress_type=deflate>\n",
      "2020-10-01 18:01:07,058 : INFO : loaded (189193, 300) matrix from <zipfile.ZipExtFile [closed]>\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "model_url = 'http://vectors.nlpl.eu/repository/20/180.zip'\n",
    "m = wget.download(model_url)\n",
    "model_file = model_url.split('/')[-1]\n",
    "with zipfile.ZipFile(model_file, 'r') as archive:\n",
    "    stream = archive.open('model.bin')\n",
    "    model = gensim.models.KeyedVectors.load_word2vec_format(stream, binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Допустим, нам интересны такие слова (пример для русского языка):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['день_NOUN', 'ночь_NOUN', 'человек_NOUN', 'семантика_NOUN', 'студент_NOUN', 'студент_ADJ']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попросим у модели 10 ближайших соседей для каждого слова и коэффициент косинусной близости для каждого:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-01 18:01:12,321 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "день_NOUN\n",
      "неделя_NOUN 0.7375995516777039\n",
      "день_PROPN 0.7067667245864868\n",
      "месяц_NOUN 0.7037326693534851\n",
      "час_NOUN 0.6643949747085571\n",
      "утро_NOUN 0.6526744961738586\n",
      "вечер_NOUN 0.6038411855697632\n",
      "сутки_NOUN 0.5923081040382385\n",
      "воскресенье_NOUN 0.5842781066894531\n",
      "полдень_NOUN 0.5743687152862549\n",
      "суббота_NOUN 0.5345946550369263\n",
      "\n",
      "\n",
      "ночь_NOUN\n",
      "ночь_PROPN 0.8310786485671997\n",
      "вечер_NOUN 0.7183678150177002\n",
      "рассвет_NOUN 0.696594774723053\n",
      "ночи_NOUN 0.6920218467712402\n",
      "полночь_NOUN 0.6704976558685303\n",
      "ночь_VERB 0.6615264415740967\n",
      "утро_NOUN 0.6263935565948486\n",
      "ночной_ADJ 0.6024709343910217\n",
      "полдень_NOUN 0.5835086107254028\n",
      "сумерки_NOUN 0.5671443343162537\n",
      "\n",
      "\n",
      "человек_NOUN\n",
      "человек_PROPN 0.7850059270858765\n",
      "человеческий_ADJ 0.5915265679359436\n",
      "существо_NOUN 0.573693037033081\n",
      "народ_NOUN 0.5354466438293457\n",
      "личность_NOUN 0.5296981930732727\n",
      "человечество_NOUN 0.5282931327819824\n",
      "человкъ_PROPN 0.5047001838684082\n",
      "индивидуум_NOUN 0.5000404715538025\n",
      "нравственный_ADJ 0.4972919821739197\n",
      "потому_ADV 0.49293625354766846\n",
      "\n",
      "\n",
      "семантика_NOUN\n",
      "семантический_ADJ 0.8019332885742188\n",
      "синтаксический_ADJ 0.7569340467453003\n",
      "модальный_ADJ 0.7296056747436523\n",
      "семантически_ADV 0.7209396958351135\n",
      "смысловой_ADJ 0.7159027457237244\n",
      "референция_NOUN 0.7135108709335327\n",
      "ноэтический_ADJ 0.7080267071723938\n",
      "языковой_ADJ 0.7067198753356934\n",
      "лингвистический_ADJ 0.6928658485412598\n",
      "предикат_NOUN 0.6877546906471252\n",
      "\n",
      "\n",
      "студент_NOUN\n",
      "преподаватель_NOUN 0.6743764877319336\n",
      "студенческий_ADJ 0.6486333608627319\n",
      "университетский_ADJ 0.6442699432373047\n",
      "заочник_NOUN 0.6423174142837524\n",
      "первокурсник_NOUN 0.6409708261489868\n",
      "курсистка_NOUN 0.636457085609436\n",
      "дипломник_NOUN 0.6341054439544678\n",
      "аспирант_NOUN 0.6337909698486328\n",
      "университет_NOUN 0.6302101612091064\n",
      "студентка_NOUN 0.6299036741256714\n",
      "\n",
      "\n",
      "студент_ADJ is not present in the model\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    # есть ли слово в модели? Может быть, и нет\n",
    "    if word in model:\n",
    "        print(word)\n",
    "        # выдаем 10 ближайших соседей слова:\n",
    "        for i in model.most_similar(positive=[word], topn=10):\n",
    "            # слово + коэффициент косинусной близости\n",
    "            print(i[0], i[1])\n",
    "        print('\\n')\n",
    "    else:\n",
    "        # Увы!\n",
    "        print(word + ' is not present in the model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Находим косинусную близость пары слов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.22025342\n"
     ]
    }
   ],
   "source": [
    "print(model.similarity('человек_NOUN', 'обезьяна_NOUN'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Более сложные операции над векторами"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Помимо более простых операций над векторами (нахождение косинусной близости между двумя векторами и ближайших соседей вектора) **gensim** позволяет выполнять и более сложные операции над несколькими векторами. Так, например, мы можем найти лишнее слово в группе. Лишним словом является то, вектор которого наиболее удален от других векторов слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "картофель_NOUN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/gensim/models/keyedvectors.py:877: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n"
     ]
    }
   ],
   "source": [
    "print(model.doesnt_match('яблоко_NOUN груша_NOUN виноград_NOUN банан_NOUN лимон_NOUN картофель_NOUN'.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также можно складывать и вычитать вектора нескольких слов. Например, сложив два вектора и вычтя из них третий вектор, мы можем решить своеобразную пропорцию. Подробнее о семантических пропорциях вы можете прочитать в [материале Системного Блока](https://vk.com/@sysblok-vo-chto-prevraschaetsya-zhizn-bez-lubvi)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "гамбургер_NOUN\n"
     ]
    }
   ],
   "source": [
    "print(model.most_similar(positive=['пицца_NOUN', 'россия_NOUN'], negative=['италия_NOUN'])[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Заключение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом тьюториале мы постарались разобраться с тем, как работать с семантическими векторными моделями и библиотекой **gensim**. Теперь вы можете:\n",
    "* осуществлять предобработку текстовых данных, что может пригодиться во многих задачах обработки естественного языка;\n",
    "* тренировать векторные семантические модели. Формат моделей совместим с моделями, представленными на веб-сервисе **RusVectōrēs**;\n",
    "* осуществлять простые операции над векторами слов.\n",
    "\n",
    "Мы надеемся, что этот тьюториал поможет нашим читателям поглубже окунуться в мир дистрибутивной семантики и использовать эти инструменты в своей работе!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
